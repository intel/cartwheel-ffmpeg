From 60cf718b40d6d13a3b7e00e8de11b7aa9118d031 Mon Sep 17 00:00:00 2001
From: Ting Fu <ting.fu@intel.com>
Date: Mon, 11 Apr 2022 23:20:19 +0800
Subject: [PATCH 41/46] libavfi/dnn: enble ipex for LibTorch backend GPU
 inference support

IPEX has not been officially released, this patch should be in cartwheel
repo only.

Signed-off-by: Ting Fu <ting.fu@intel.com>
---
 configure                             |  2 +-
 libavfilter/dnn/dnn_backend_torch.cpp | 13 +++++++++++++
 2 files changed, 14 insertions(+), 1 deletion(-)

diff --git a/configure b/configure
index 4ac5bf1519..3d97f4bcf5 100755
--- a/configure
+++ b/configure
@@ -6677,7 +6677,7 @@ enabled libopus           && {
 }
 enabled libplacebo        && require_pkg_config libplacebo "libplacebo >= 4.192.0" libplacebo/vulkan.h pl_vulkan_create
 enabled libpulse          && require_pkg_config libpulse libpulse pulse/pulseaudio.h pa_context_new
-enabled libtorch          && add_cppflags -D_GLIBCXX_USE_CXX11_ABI=0 && check_cxxflags -std=c++14 && require_cpp libtorch torch/torch.h "torch::Tensor" -ltorch -lc10 -ltorch_cpu -lstdc++ -lpthread
+enabled libtorch          && add_cppflags -D_GLIBCXX_USE_CXX11_ABI=0 && check_cxxflags -std=c++14 && require_cpp libtorch torch/torch.h "torch::Tensor" -ltorch -lc10 -ltorch_cpu -lstdc++ -lpthread -Wl,--no-as-needed -lipex_gpu_core -Wl,--as-needed
 enabled librabbitmq       && require_pkg_config librabbitmq "librabbitmq >= 0.7.1" amqp.h amqp_new_connection
 enabled librav1e          && require_pkg_config librav1e "rav1e >= 0.5.0" rav1e.h rav1e_context_new
 enabled librist           && require_pkg_config librist "librist >= 0.2.7" librist/librist.h rist_receiver_create
diff --git a/libavfilter/dnn/dnn_backend_torch.cpp b/libavfilter/dnn/dnn_backend_torch.cpp
index 69556dd96c..fde0a022ae 100644
--- a/libavfilter/dnn/dnn_backend_torch.cpp
+++ b/libavfilter/dnn/dnn_backend_torch.cpp
@@ -223,6 +223,14 @@ DNNModel *ff_dnn_load_model_th(const char *model_filename, DNNFunctionType func_
     c10::Device device = c10::Device(ctx->options.device_name);
     if (device.is_cpu()) {
         ctx->options.device_type = torch::kCPU;
+    } else if (device.is_xpu()) {
+        if (!at::hasXPU()) {
+            av_log(ctx, AV_LOG_ERROR, "No XPU device found\n");
+            return NULL;
+        }
+        at::detail::getXPUHooks().initXPU();
+        at::deviceExclusiveCheck();
+        ctx->options.device_type = torch::kXPU;
     } else {
         av_log(ctx, AV_LOG_ERROR, "Not supported device:\"%s\"\n", ctx->options.device_name);
         goto fail;
@@ -335,6 +343,8 @@ static int fill_model_input_th(THModel *th_model, THRequestItem *request)
     }
     *infer_request->input_tensor = torch::from_blob(input.data, {1, 1, 3, input.height, input.width},
                                                     torch::kFloat32);
+    if (infer_request->input_tensor->device() != ctx->options.device_type)
+        *infer_request->input_tensor = infer_request->input_tensor->to(ctx->options.device_type);
     return 0;
 
 err:
@@ -393,6 +403,9 @@ static void infer_completion_callback(void *args) {
     switch (th_model->model->func_type) {
     case DFT_PROCESS_FRAME:
         if (task->do_ioproc) {
+            //post process can only deal with CPU memory.
+            if (output->device() != torch::kCPU)
+                *output = output->to(torch::kCPU);
             outputs.data = output->data_ptr();
             if (th_model->model->frame_post_proc != NULL) {
                 th_model->model->frame_post_proc(task->out_frame, &outputs, th_model->model->filter_ctx);
-- 
2.25.1

