From b4f9ee0635742aeb94f55f9038131348b4d289b0 Mon Sep 17 00:00:00 2001
From: Ting Fu <ting.fu@intel.com>
Date: Mon, 11 Apr 2022 23:20:19 +0800
Subject: [PATCH] libavfi/dnn: enble ipex for LibTorch backend GPU inference
 support

IPEX has not been officially released, this patch should be in cartwheel
repo only.

Signed-off-by: Ting Fu <ting.fu@intel.com>
---
 configure                             |  2 +-
 libavfilter/dnn/dnn_backend_torch.cpp | 24 ++++++++++++++++++++----
 2 files changed, 21 insertions(+), 5 deletions(-)

diff --git a/configure b/configure
index e1ea21c697..b2f420a6f3 100755
--- a/configure
+++ b/configure
@@ -7000,7 +7000,7 @@ enabled libtensorflow     && require libtensorflow tensorflow/c/c_api.h TF_Versi
 enabled libtesseract      && require_pkg_config libtesseract tesseract tesseract/capi.h TessBaseAPICreate
 enabled libtheora         && require libtheora theora/theoraenc.h th_info_init -ltheoraenc -ltheoradec -logg
 enabled libtls            && require_pkg_config libtls libtls tls.h tls_configure
-enabled libtorch          && check_cxxflags -std=c++17 && require_cpp libtorch torch/torch.h "torch::Tensor" -ltorch -lc10 -ltorch_cpu -lstdc++ -lpthread
+enabled libtorch          && check_cxxflags -std=c++17 && require_cpp libtorch torch/torch.h "torch::Tensor" -ltorch -lc10 -ltorch_cpu -lstdc++ -lpthread -Wl,--no-as-needed -lintel-ext-pt-gpu -Wl,--as-needed
 enabled libtwolame        && require libtwolame twolame.h twolame_init -ltwolame &&
                              { check_lib libtwolame twolame.h twolame_encode_buffer_float32_interleaved -ltwolame ||
                                die "ERROR: libtwolame must be installed and version must be >= 0.3.10"; }
diff --git a/libavfilter/dnn/dnn_backend_torch.cpp b/libavfilter/dnn/dnn_backend_torch.cpp
index abdef1f178..054a73a3f8 100644
--- a/libavfilter/dnn/dnn_backend_torch.cpp
+++ b/libavfilter/dnn/dnn_backend_torch.cpp
@@ -43,6 +43,7 @@ typedef struct THModel {
     SafeQueue *request_queue;
     Queue *task_queue;
     Queue *lltask_queue;
+    c10::DeviceType device_type;
 } THModel;
 
 typedef struct THInferRequest {
@@ -213,6 +214,8 @@ static int fill_model_input_th(THModel *th_model, THRequestItem *request)
     *infer_request->input_tensor = torch::from_blob(input.data,
         {1, input.dims[channel_idx], input.dims[height_idx], input.dims[width_idx]},
         deleter, torch::kFloat32);
+    if (infer_request->input_tensor->device() != ctx->options.device_type)
+        *infer_request->input_tensor = infer_request->input_tensor->to(ctx->options.device_type);
     return 0;
 
 err:
@@ -285,6 +288,9 @@ static void infer_completion_callback(void *args) {
     switch (th_model->model->func_type) {
     case DFT_PROCESS_FRAME:
         if (task->do_ioproc) {
+            //post process can only deal with CPU memory.
+            if (output->device() != torch::kCPU)
+                *output = output->to(torch::kCPU);
             outputs.scale = 255;
             outputs.data = output->data_ptr();
             if (th_model->model->frame_post_proc != NULL) {
@@ -432,14 +438,24 @@ static DNNModel *dnn_load_model_th(DnnContext *ctx, DNNFunctionType func_type, A
     th_model->ctx = ctx;
 
     c10::Device device = c10::Device(device_name);
-    if (!device.is_cpu()) {
-        av_log(ctx, AV_LOG_ERROR, "Not supported device:\"%s\"\n", device_name);
-        goto fail;
-    }
+    if (device.is_cpu()) {
+         th_model->device_type = torch::kCPU;
+    } else if (device.is_xpu()) {
+        if (!at::hasXPU()) {
+            av_log(ctx, AV_LOG_ERROR, "No XPU device found\n");
+            return NULL;
+        }
+        at::detail::getXPUHooks().initXPU();
+        th_model->device_type = torch::kXPU;
+     } else {
+         av_log(ctx, AV_LOG_ERROR, "Not supported device:\"%s\"\n", device_name);
+         goto fail;
+     }
 
     try {
         th_model->jit_model = new torch::jit::Module;
         (*th_model->jit_model) = torch::jit::load(ctx->model_filename);
+        th_model->jit_model->to(th_model->device_type);
     } catch (const c10::Error& e) {
         av_log(ctx, AV_LOG_ERROR, "Failed to load torch model\n");
         goto fail;
-- 
2.34.1

