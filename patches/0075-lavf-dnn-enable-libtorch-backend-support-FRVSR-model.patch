From 8c9fb47e8ca7cc33cc6823545e74d46bbd85f1b0 Mon Sep 17 00:00:00 2001
From: Ting Fu <ting.fu@intel.com>
Date: Wed, 29 Jun 2022 07:46:44 +0000
Subject: [PATCH 17/50] lavf/dnn: enable libtorch backend support FRVSR model

Signed-off-by: Ting Fu <ting.fu@intel.com>
---
 libavfilter/dnn/dnn_backend_torch.cpp | 34 ++++++++++++++++++++++++++-
 1 file changed, 33 insertions(+), 1 deletion(-)

diff --git a/libavfilter/dnn/dnn_backend_torch.cpp b/libavfilter/dnn/dnn_backend_torch.cpp
index 7891ae4386..668b6b5ef3 100644
--- a/libavfilter/dnn/dnn_backend_torch.cpp
+++ b/libavfilter/dnn/dnn_backend_torch.cpp
@@ -46,6 +46,8 @@ typedef struct THContext {
     THOptions options;
 } THContext;
 
+typedef enum {UNKNOWN_MODEL = -1, BASICVSR, FRVSR} ModelType;
+
 typedef struct THModel {
     THContext ctx;
     DNNModel *model;
@@ -53,6 +55,7 @@ typedef struct THModel {
     SafeQueue *request_queue;
     Queue *task_queue;
     Queue *lltask_queue;
+    ModelType model_type;
 } THModel;
 
 typedef struct THInferRequest {
@@ -246,6 +249,7 @@ static int th_start_inference(void *args)
     THContext *ctx = NULL;
     std::vector<torch::jit::IValue> inputs;
     torch::NoGradGuard no_grad;
+    c10::DeviceType device_type;
 
     if (!request) {
         av_log(NULL, AV_LOG_ERROR, "THRequestItem is NULL\n");
@@ -256,6 +260,7 @@ static int th_start_inference(void *args)
     task = lltask->task;
     th_model = (THModel *)task->model;
     ctx = &th_model->ctx;
+    device_type = ctx->options.device_type;
 
     if (ctx->options.optimize)
         torch::jit::setGraphExecutorOptimize(true);
@@ -268,7 +273,24 @@ static int th_start_inference(void *args)
     }
     inputs.push_back(*infer_request->input_tensor);
 
-    *infer_request->output = th_model->jit_model->forward(inputs).toTensor();
+    if (th_model->model_type == FRVSR) {
+        auto size = infer_request->input_tensor->sizes();
+        int height = size[2];
+        int width  = size[3];
+        torch::Tensor lr_prev = torch::zeros({1, 3, height, width}, torch::TensorOptions().dtype(torch::kFloat32)
+                                                                                          .device(device_type));
+        torch::Tensor hr_prev = torch::zeros({1, 3, height * 4, width * 4}, torch::TensorOptions().dtype(torch::kFloat32)
+                                                                                                  .device(device_type));
+        inputs.push_back(lr_prev);
+        inputs.push_back(hr_prev);
+    }
+
+    auto outputs = th_model->jit_model->forward(inputs);
+    if (th_model->model_type == FRVSR) {
+        *infer_request->output = outputs.toTuple()->elements()[0].toTensor();
+    } else {
+        *infer_request->output = outputs.toTensor();
+    }
 
     return 0;
 }
@@ -435,6 +457,7 @@ static DNNModel *dnn_load_model_th(const char *model_filename, DNNFunctionType f
     THModel *th_model = NULL;
     THRequestItem *item = NULL;
     THContext *ctx;
+    torch::jit::NameTensor first_param;
 
     model = (DNNModel *)av_mallocz(sizeof(DNNModel));
     if (!model) {
@@ -480,6 +503,7 @@ static DNNModel *dnn_load_model_th(const char *model_filename, DNNFunctionType f
         av_log(ctx, AV_LOG_ERROR, "Failed to load torch model\n");
         goto fail;
     }
+    first_param = *th_model->jit_model->named_parameters().begin();
 
     th_model->request_queue = ff_safe_queue_create();
     if (!th_model->request_queue) {
@@ -515,6 +539,14 @@ static DNNModel *dnn_load_model_th(const char *model_filename, DNNFunctionType f
         goto fail;
     }
 
+    if (!first_param.name.find("fnet")) {
+        th_model->model_type = FRVSR;
+    } else if (!first_param.name.find("spynet")) {
+        th_model->model_type = BASICVSR;
+    } else {
+        th_model->model_type = UNKNOWN_MODEL;
+    }
+
     model->get_input = &get_input_th;
     model->get_output = &get_output_th;
     model->options = NULL;
-- 
2.34.1

